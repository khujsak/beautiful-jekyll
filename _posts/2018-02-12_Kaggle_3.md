

```python

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import networkx as nx
import scipy.spatial as ss
from numpy.linalg import inv
import tess as ts

     

def get_xyz_data(filename):
    pos_data = []
    lat_data = []
    with open(filename) as f:
        for line in f.readlines():
            x = line.split()
            if x[0] == 'atom':
                pos_data.append([np.array(x[1:4], dtype=np.float),x[4]])
            elif x[0] == 'lattice_vector':
                lat_data.append(np.array(x[1:4], dtype=np.float))
    return pos_data, np.array(lat_data)

zz=2100
structure_index=zz

train=pd.read_csv('train.csv')




idx = train.id.values[structure_index]
fn = "./train/{}/geometry.xyz".format(idx)
train_xyz, train_lat = get_xyz_data(fn)


d = {'structure': [train.id.values[structure_index]]}


df = pd.DataFrame(data=d)

A=np.transpose(train_lat)
B = inv(A)

xyz=[]

for i in enumerate(train_xyz):
    
#    print(i[1][0])
    
    r = np.matmul(B, i[1][0])
    xyz.append(r)



cntr=ts.Container(xyz, limits=((0,0,0),(1,1,1)), periodic=True)    
```

This is a continuation from Part 1 where I showed you, given a list of atomic positions and the basis vectors of the unit cell, one could produce a periodic tesselation of that crystal using the fantastic voro++ package.  The container object from this tesslation carries a lot of useful information, including the list of neighboring cells, the face areas of those cells, and the list of vertices for each area.  When we left off we had just converted the vertices back to real space.In this post I'm going to assume you've followed along with part 1.

Go ahead and load structure 2100, convert the atomic positions into reduced form, and created a tess container with periodic boundary conditions.  We will be calculating a lot of features based on this particular tesselation, and for the most part we will be using the amount of shared face area is a weighting factor for differences in elemental properties.  This is motivated by the fact that elements which share a large degree of available surface area are likely participating in a high degree of bonding.

Within the container object we have two paritcularly important lists of lists, the positions of the verticies in each cell, and the sets of vertices which contribute to each face.  Thus, we first need to convert all of the verticies from reduced coordinates back into real space.  Using these new coordinates and the list of which vertices correspond to each face, we will then recalculate the face areas.  We will loop through all the tesselated cells when performing this construction.

First we'll define a few helper functions to determine the unit_normal of a face with an arbitrary number of vertices, and the total area of the polygon using those vertices.  I'm using the excellent Stack Overflow response by Tom Smilack, which you can find [here](https://stackoverflow.com/questions/12642256/python-find-area-of-polygon-from-xyz-coordinates).


```python
def unit_normal(a, b, c):
    x = np.linalg.det([[1,a[1],a[2]],
         [1,b[1],b[2]],
         [1,c[1],c[2]]])
    y = np.linalg.det([[a[0],1,a[2]],
         [b[0],1,b[2]],
         [c[0],1,c[2]]])
    z = np.linalg.det([[a[0],a[1],1],
         [b[0],b[1],1],
         [c[0],c[1],1]])
    magnitude = (x**2 + y**2 + z**2)**.5
    return (x/magnitude, y/magnitude, z/magnitude)


#area of polygon poly
def poly_area(poly):
    if len(poly) < 3: # not a plane - no area
        return 0
    total = [0, 0, 0]
    N = len(poly)
    for i in range(N):
        vi1 = poly[i]
        vi2 = poly[(i+1) % N]
        prod = np.cross(vi1, vi2)
        total[0] += prod[0]
        total[1] += prod[1]
        total[2] += prod[2]
    result = np.dot(total, unit_normal(poly[0], poly[1], poly[2]))
    return abs(result/2)
```

Here we'll read out the vertices and face_vertices out of the container into two temporary lists, and define an empty list for the real space face areas (face_areas_r).  Note that we perform the conversion from reduced to real space inside the for loop, and append the list of new areas for each cell into face_areas_r.


```python
vertices=np.array([v.vertices() for v in cntr])
face_vertices=[v.face_vertices() for v in cntr]

face_areas_r=[]

for j in range(len(face_vertices)):    
    areas=[]    
    tmp=np.array(vertices[j])
    
    for i in range(len(face_vertices[j])):
        corrected=[]        
        dummy=tmp[face_vertices[j][i], :]
        
        for k in range(len(dummy)):    
            corrected.append(np.matmul(A, dummy[k]))
            
        areas.append(poly_area(corrected))
        
    face_areas_r.append(areas)
```

We now have the list of areas for every face in each tesselated cell in real space.  This will allow for direct comparison between different unit cells/geometries.  From [Ward et al.](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.96.024104) some simple immediate parameters we can calculate are some statistics from the coordination each atom experiences in the structure (ie, number of faces shared for each tesselated cell).  This tells us something about the statistical bonding experienced by each atom in the material.

However, some faces may have an exceptionally small shared area, and do not truly represent a bonding state.  Thus, a naive counting of faces may significantly overestimate the local coordination of each element.  Instead, for each tesselated cell we'll calculate the square of the summed face areas divided by the sum of the square.  Then we will calculate the maximum, minimum, mean, and mean absolute deviation for each cell in the structure.


```python
coord=[]

for v in face_areas_r:
    
    num=np.sum(v)**2
    denom=np.sum(np.power(v, 2))
    coord.append(num/denom)
    
features_coord=[np.max(coord), np.min(coord), np.mean(coord), np.sum(np.abs(coord-np.mean(coord)))/len(coord)]
```

Next, let's consider the statistical deviation of elemental properties within the tesselation.  For example, given a central Oxygen atom, we can consider the electronegativity difference of all face sharing elements.  To penalize the contribution of neighbors with very small shared areas, we'll normalize all of the descriptors by the area of the shared face.

Let's define some dictionaries with interesting elemental properties:


```python
electronegativities={"Al":1.61, "In":1.78, "Ga": 1.81, "O":3.44}
electron_affinities = {"Al" : 0.432, "In": 0.3, "Ga": 0.43,  "O": 1.461 }
covalent_radius = {"Al" : 125, "In": 155, "Ga": 130,  "O": 60 }
valence = {"Al" : 3, "In": 3, "Ga": 3,  "O": 2 }
melting_T = {"Al" :660.32, "In": 156.6, "Ga": 29.76,  "O": -218.3 }
mendeleev_num={"Al" :73, "In": 75, "Ga": 74,  "O": 87 }
atomic_weight={"Al" :26.98, "In": 114.8, "Ga": 69.72,  "O": 15.99 }
effective_local_charge={"Al" :4.066, "In": 8.470, "Ga": 6.222,  "O": 4.453 }
heat_capacity={'Al': 24.2, 'Ga': 25.86, 'In': 26.74, 'O': 29.378}   
heat_of_fusion={'Al': 10700.0, 'Ga': 5590.0, 'In': 3260.0, 'O': 222.0}
heat_of_vaporization={'Al': 294, 'Ga': 254, 'In': 231.8, 'O':  6.820}  
first_ionization = {'Al': 577.5, 'Ga': 578.8, 'In': 558.3, 'O': 1313.9}
second_ionization = {'Al': 1816.7, 'Ga': 1979.3, 'In': 1820.7, 'O': 3388.3}    
third_ionization = {'Al': 2744.8, 'Ga': 2963, 'In': 2704, 'O': 5300.5}      
thermal_conductivity = {'Al': 235.0, 'Ga': 29.0, 'In': 82.0, 'O': 0.02658}
molar_volume={'Al': 10.00, 'Ga': 11.803, 'In': 15.76, 'O': 22.4134}
chemical_hardness={'Al': 2.77, 'Ga': 2.9, 'In': 2.8, 'O': 6.08}  
polarizability = {'Al': 57.74, 'Ga': 51.4, 'In': 68.7, 'O': 6.1}  
```

And let's define a helper function, which given a list of atoms, neighbors for each atom, and a look-up dictionary of a particular property of interest, will return the statistical properties for the entire geometry.


```python
def calculate_local_properties(train_array, dictionary, neighbors, core_atom, face_areas_r):
    
    dict_core=[dictionary[x] for x in core_atom]
    dict_tot=[]
    
    for i in range(len(face_areas_r)):
        
        element_list_neigh=train_array[neighbors[i], 1]
        areas=face_areas_r[i]
        values_n=np.array([dictionary[x] for x in element_list_neigh])
        dict_tot.append(np.sum(areas*np.abs(values_n-dict_core[i]))/np.sum(areas))
        
    features=[np.max(dict_tot), np.min(dict_tot), np.mean(dict_tot), np.sum(np.abs(dict_tot-np.mean(dict_tot)))/len(dict_tot)]
    return features
```

Given a particular core atom, the function will grab all of its neighbors, the shared face areas for those atoms, and the corresponding elemental property the core atom and each neighbor.  The difference between the core atom and neighbor atom property is multiplied by the face shared area, summed, and then normalized by the total face areas of that particular cell.

Finally, the maximum, minimum, mean, and mean absolute deviation of the summed normalized differences for each cell are tabulated and returned.  We can then loop through our predefined dictionaries:


```python
local_properties=[electronegativities,
                  electron_affinities,
                  covalent_radius,
                  valence,
                  melting_T,
                  mendeleev_num,
                  atomic_weight,
                  polarizability]

neighbors=[v.neighbors() for v in cntr]
train_array=np.array(train_xyz)
core_atom=train_array[:,1]



for dictionary in local_properties:
    
    property_name=[ k for k,v in locals().items() if v is dictionary][0]    
    features=calculate_local_properties(train_array, dictionary, neighbors, core_atom, face_areas_r)
```

Now let's consider the actual bonding distances between participating atoms.  It's relatively intuitive to imagine that if all the atoms in a particular material have very long bonding distances, this structure is unlikely to be very stable.  The energy contained with a bond scales as a power of distance (depending on the potential involved), so having some statistical descriptors of the overall bonding scale within a structure is likely to be a very powerful predictor of formation energy.

We discussed in part 1 that it is possible to define a network of bonding elements from the raw atomic positions in real space, simply using some cutoff to specify connectivity.  This cutoff could be set against some physically inspired value, like covalent radii.  One can then simply calculate the mean values of the edges of this weighted graph.  

However, consider if two atoms are relatively close due to a very dense unit cell, but share a very small face area.  We should not consider this bonding distance when tabulating statistics about bonding distances as these elements are unlikely to actual participate in any kind of bonding in the real material.  Our tesselation provides us a facile way to weight the relative distances between elements.  

Consider the following short code, which computes the distance between a core atom and all of its neighbors, and weights it by the normalized shared face area.  Similar to before, we can then compute the maximum, mininum, mean, and mean absolute deviation as features in our model.


```python
i=0
bond_dist=[]

for atom in enumerate(train_xyz):
    
    neighbor_atoms=train_array[neighbors[i], 0]
    dist=[]

    for k in range(len(neighbor_atoms)):
        dist.append( np.linalg.norm(atom[1][0]-neighbor_atoms[k]))
        
    bond_dist.append(np.dot(dist, face_areas_r[i])/np.sum(face_areas_r[i]))   
    i+=1
    
features=[np.max(bond_dist), np.min(bond_dist), np.mean(bond_dist), np.sum(np.abs(bond_dist-np.mean(bond_dist)))/len(bond_dist)]        
```

That's all for this time!  In part 3, I'll show you how to calculate some more exotic features based on network analysis of the graph defined by the neighboring voronoi cells.

Welcome to Part 3 of my Voronoi based solution to the NOMAD 2018 Transparent Conducting Oxide Kaggle.  As before, our goal is to correctly predict the formation energies (ie, stability) and band gaps (ie, optical transparency) of different candidate sesquioxide materials.  In the previous two parts, we have calculated a tesselation of the reduced coordinate form of the geometry file (unit cell) and converted the areas of the polygonal faces back to real space.  We then engineered several descriptors from the structure of these files.

In this part, we'll calculate some more advanced geometric features of these tesselations and begin some network analysis of the weighted graph defined by the neighbor lists and face areas.

We'll pick up right where we left off, and calculate some statistical descriptors of the volume of each cell.  Voro++ will give us the volume of each cell directly, but this is respect to the reduced coordinates of the cell.  We need to recalculate volumes in real space, for which we will use the Convex Hull implementation in scipy.


```python
import scipy.spatial as ss

vertices=np.array([v.vertices() for v in cntr])

cell_vols_r=[]

for j in range(len(vertices)):
         
    corrected=[]     
    
    for k in range(len(vertices[j])):
           
        corrected.append(np.matmul(A, vertices[j][k]))
        
    hull=ss.ConvexHull(corrected)     
    cell_vols_r.append(hull.volume)
        
mad_cell_volume=np.sum(np.abs(cell_vols_r-np.mean(cell_vols_r)))/(len(cell_vols_r)*np.mean(cell_vols_r))        
```

As we did before, we take the list of vertices for each cell, convert from reduced to real space, and calculate the convex hull enclosed by this points.  Appending the volume to a list cells_vols_r, we can then calculate as many parameters as desired for the structure.  In this case we have only calculated the mean absolute deviation.

Let's also consider the maximum packing efficiency of each cell in the crystal.  One can think about the largest sphere which could be contained within a particular volume, in this case the tessalated cell.  To calculate this, we first need to determine the center point of each face in a cell, which we do below:


```python
face_centers_r=[]

for j in range(len(face_vertices)):
     
    centers=[]
    tmp=np.array(vertices[j])
    
    for i in range(len(face_vertices[j])):
        corrected=[]        
        dummy=tmp[face_vertices[j][i], :]
        
        for k in range(len(dummy)):
            
            corrected.append(np.matmul(A, dummy[k]))
        
        centers.append(np.mean(corrected, axis=0))
        
    face_centers_r.append(centers)
```

Now, we can calculate the minimum distance between the atom position in the cell and the center of each face.  If we were to inflate the atom in the cell adn record the maximum radius before the atom contacts a face, due to the construction of the voronoi tesselation (perpindicular bisectors) this would be the closest center of any polygonal face.  We calculate this minimum distance for all cells and record the volume of such a sphere:


```python
maximum_packing=0

for i,atom in enumerate(train_xyz):
    
    neighbor_atoms=face_centers_r[i]
    dist=[]

    for k in range(len(neighbor_atoms)):
        
        dist.append(np.linalg.norm(atom[1][0]-neighbor_atoms[k]))
  
    maximum_packing+=4/3*(np.pi*np.power(np.min(dist), 3))

maximum_packing=maximum_packing/np.sum(cell_vols_r)
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-52-62a64238a0bf> in <module>()
          8     for k in range(len(neighbor_atoms)):
          9 
    ---> 10         dist.append(np.linalg.norm(atom[1][0]-neighbor_atoms[k]))
         11 
         12     maximum_packing+=4/3*(np.pi*np.power(np.min(dist), 3))
    

    TypeError: ufunc 'subtract' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')


Now we're going to shift gears somewhat radically.  So far, we have only considered constructing features based on each cell independently, and then taking statistical metrics like mean, min, max, etc. of the ensemble of cells.  Let's consider how we might be able to construct features based on the arrangement of polygonal faces between multiple sets of cells simultaneously.  We will thus shift from considering local order parameters between single neighbors, to short range ordering descriptors, which capture how multiple cells are ordered with respect to one another.  

First, let's consider constructing a weighted undirected graph of this crystal using the list of neighbors using the excellent python package networkx.


```python
import networkx as nx

G=nx.Graph()
l=[]    
for i,atom in enumerate(core_atom, 0):
    
    G.add_node(i, element=atom)
    
    l=neighbors[i]
    l=[(i,x) for x in l]
    G.add_edges_from(l)    
        
labels = dict(zip([v for v in range(len(train_xyz))], core_atom))

d = dict([(y,x+1) for x,y in enumerate(sorted(set(core_atom)))])
node_color=[d[x] for x in core_atom]

                     
nx.draw_networkx(G,pos=nx.spectral_layout(G), labels=labels, node_color=node_color, vmin=-1, vmax=3, style='dotted')
limits=plt.axis('off')   
```


![png](output_23_0.png)


Without even including the weighted face areas, we can already tell there is some order in the connectivity of the graph from the spectral representation.  Each metal cation appears to be very strongly coordinated with the oxygen atoms (which makes sense).  How can we express this short range ordering of the graph in an intuitive fashion?  Taking inspiration from graph kernels, let's consider the probability of ending up on any particular element after taking a series of random walks through the crystal.  If the atoms were truly randomly distributed (and thus likely unstable), we would expect that our odds of ending on any particular element (Ga, In, O, or Al) would be the atomic fraction of that element in the crystal.  

Following from [Ward et al.](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.96.024104), we define an approximation the Warren-Crowley ordering parameters as follows:

$$ \alpha(t,s) = 1 - \frac{\sum_p w_p \delta(t-t_p)}{x_t} $$

Where $t$ is the elemental type in the crystal, $s$ is the shell or path length we consider, and $p$ is the set of simple non-backtracking paths in the crystal that end on element $p$.  This entire value is weighted by the atomic fraction $x_t$ and multiplied by a weighting term related to the face area of each step:

$$ w_p = \frac{A_n}{\sum_a A_a - \sum_b B_b} $$

For one step, $n$, in a particular path $p$, we divide the area of the face $A_n$ by the face area of all possible steps, $A_a$, subtracting the area of backtracking paths, $B_b$.

For the first shell (path lengths equal to one), we can simply iterate through all atoms in our crystal and perform the area weighting described above only when a neighbor is the same elemental type as the core atom.  Thus for an oxygen atom, we sum up all the face areas where the neighbors are also oxygen, and divide by both the total face areas and the atomic fraction of Oxygen in the crystal.


```python
unique_elements, atomic_fraction = np.unique(train_array[:,1], return_counts= True)

atomic_fraction=atomic_fraction/len(train_array)
oxygen_fraction_index=np.argwhere(unique_elements=='O')
aluminum_fraction_index=np.argwhere(unique_elements=='Al')
gallium_fraction_index=np.argwhere(unique_elements=='Ga')
indium_fraction_index=np.argwhere(unique_elements=='In')

first_order_param_O=[]
first_order_param_In=[]
first_order_param_Ga=[]
first_order_param_Al=[]

for i in range(len(train_xyz)):
    
    #For each cell
    element=train_array[:,1][i]
    element_list_neigh=train_array[neighbors[i], 1]
    areas=face_areas_r[i]
    
    numerator_area_indices=np.argwhere(element_list_neigh==element)
    
    if element=='O':        
        first_order_param_O.append(1-(np.sum([areas[int(v)] for v in numerator_area_indices]))/(np.sum(areas)*(atomic_fraction[oxygen_fraction_index])))   
    if element=='Ga':
        first_order_param_Ga.append(1-(np.sum([areas[int(v)] for v in numerator_area_indices]))/(np.sum(areas)*(atomic_fraction[gallium_fraction_index])))
    if element=='In':
        first_order_param_In.append(1-(np.sum([areas[int(v)] for v in numerator_area_indices]))/(np.sum(areas)*(atomic_fraction[indium_fraction_index])))
    if element=='Al':
        first_order_param_Al.append(1-(np.sum([areas[int(v)] for v in numerator_area_indices]))/(np.sum(areas)*(atomic_fraction[aluminum_fraction_index])))

if bool(first_order_param_O)==True:
    first_order_param_O=np.mean(np.abs(first_order_param_O))
    
if bool(first_order_param_Ga)==True:
    first_order_param_Ga=np.mean(np.abs(first_order_param_Ga)) 
        
if bool(first_order_param_In)==True:
    first_order_param_In=np.mean(np.abs(first_order_param_In)) 
       
if bool(first_order_param_Al)==True:
    first_order_param_Al=np.mean(np.abs(first_order_param_Al))  
    

print("Ordering Parameters for O:{}, Ga:{}, Al:{}, In:{}".format(first_order_param_O, first_order_param_Ga, first_order_param_Al, first_order_param_In))
```

    Ordering Parameters for O:0.13719721063171933, Ga:[], Al:0.8496417209957086, In:1.0
    

For the second and third ordering parameters, we need to iterate through all possible paths of length two and three and calculate according to the above formula.  Let's define a helper function, which given a target node in our graph, a path length cutoff, and the total list of all neighbors and face areas, can return the path weighting term defined above.  For enumerating paths we'll simply use the built in command all_simple_paths within networkx, but you could do a simple breadth or depth first search.


```python
def calculate_path_weights_for_atom(target, cutoff, G, neighbors ,face_areas_r):

    w_tot=0
    # find all paths that end on our target
    # by iterative through every other atom
    #in our crystal 
    for l in range(len(face_areas_r)):
              
        paths = nx.all_simple_paths(G, source=l, target=target, cutoff=cutoff)
    
        for path in map(nx.utils.pairwise, paths):
        
            single_path=[]
            single_path.append((list(path)))
            w=1
            
            #check if path length is correct
            if len(single_path[0])==cutoff:
                
                #for each step in the path, compute weight
                for i in range(len(single_path[0])):
                    
                    tmp=single_path[0][i]
                    areas=face_areas_r[tmp[0]]
                    nn=neighbors[tmp[0]]
                    face_index=np.argwhere(np.array(nn)==tmp[1])
                    
                    #if there are no backtracking steps
                    #because we are on the first step
                    if i==0:             
                        denom=np.sum(areas)
                        num=areas[face_index[0][0]]
                        w=w*(num/denom)
                    else:
                        last=single_path[0][i-1][0]
                        last_index=np.argwhere(np.array(nn)==last)
                        denom=np.sum(areas)-areas[last_index[0][0]]
                        num=areas[face_index[0][0]]
                        w=w*num/denom
                w_tot+=w
    #this is the total weight for all paths that end on
    #our target
    return w_tot
```

Now setting our cutoff and calling the function defined above, we can return our desired ordering parameters for an arbitrary path length.


```python
path_length_cutoff=2
weight_total=np.zeros([len(face_areas_r)])

for i in range(len(face_areas_r)):    
    weight_total[i]=calculate_path_weights_for_atom(i, path_length_cutoff, G, neighbors, face_areas_r)


indicator_O=np.argwhere(train_array[:,1]=='O')
indicator_In=np.argwhere(train_array[:,1]=='In')
indicator_Ga=np.argwhere(train_array[:,1]=='Ga')
indicator_Al=np.argwhere(train_array[:,1]=='Al')


if bool(first_order_param_O)==True:
    second_order_param_O=np.mean(np.abs(1-(weight_total[indicator_O]/atomic_fraction[oxygen_fraction_index])))    
    
if bool(first_order_param_Ga)==True:
    second_order_param_Ga=np.mean(np.abs(1-(weight_total[indicator_Ga]/atomic_fraction[gallium_fraction_index])))
        
if bool(first_order_param_In)==True:  
    second_order_param_In=np.mean(np.abs(1-(weight_total[indicator_In]/atomic_fraction[indium_fraction_index])))
    
if bool(first_order_param_Al)==True:
    second_order_param_Al=np.mean(np.abs(1-(weight_total[indicator_Al]/atomic_fraction[aluminum_fraction_index])))        
```

One can similarly construct third, or even higher order parameters, by simply changing the cutoff variable.  This wraps up Part 3 of our feature extraction for the NOMAD 2018 Transparent Conducting Oxide Kaggle.  Part 4 will finalize the feature extraction and present some visualization of the features we constructed here.
